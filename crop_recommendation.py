# -*- coding: utf-8 -*-
"""Crop Recommendation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jiWPlc2N2uGdlure6_E4WtiFqzvfaN41

Joyassroy Barua
0242220005101616

Dipta Acharjee
0242220005101603

Md. Rakibur Rahman 0242220005101057

Section: 63_J2
"""

# from google.colab import drive
# drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# %matplotlib inline

"""**Data Reading**"""

df = pd.read_csv('Crop_Recommendation.csv')

# Sort Preview of data
df.head(10)

# info about dataset
df.info()

df.size, df.shape

# columns name
df.columns

df[df.duplicated()].shape[0]

# Columns Data Type
df.dtypes

df.nunique()

"""**Data Cleaning**"""

# Checking null values
df.isnull().sum()

# Checking Nan Values
df.isna().sum()

"""**Basic Analysis**"""

df.describe()

"""**Convert categorical variables to numerical**

**Label Encoding**
"""

df["Crop"].unique()

import sklearn
from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df['Crop'] = le.fit_transform(df['Crop'])
df

# import pickle
# with open("label_encoder.pkl", "wb") as f:
#     pickle.dump(le, f)
# from google.colab import files
# files.download("label_encoder.pkl")

"""**Data Vizualization**"""

# importing libraries for visualization
import plotly.express as px
import matplotlib.pyplot as plt

"""This helps you visualize how many observations correspond to each crop type.



"""

#Pie Chart: Distribution of crops in the dataset.
pie_fig = px.pie(df, names='Crop', title='This pie chart shows the distribution of different crops in your dataset.',
                 color_discrete_sequence=px.colors.qualitative.Set3)
pie_fig.update_traces(textinfo='percent+label', pull=[0.05] * len(df['Crop'].unique()))
pie_fig.show()

"""To compare the distribution of these nutrients across different crops, identifying any significant differences."""

# Box Plot: Nutrient levels (Nitrogen, Phosphorus, Potassium) across crops.
box_fig = px.box(df.melt(id_vars='Crop', value_vars=['Nitrogen', 'Phosphorus', 'Potassium']),
                 # melt fun create a single column variable
                 x='variable', y='value', color='Crop',
                 title='three key nutrients (Nitrogen, Phosphorus, and Potassium), showing how their values are distributed for different crop types',
                 labels={'variable': 'Nutriment', 'value': 'Valeur'},
                 color_discrete_sequence=px.colors.qualitative.Set3)
box_fig.show()

"""To visually compare the average nutrient levels for each crop."""

#Bar Plot: Average Nutrient Levels by Crop
mean_nutrients = df.groupby('Crop')[['Nitrogen', 'Phosphorus', 'Potassium']].mean().reset_index()

bar_nutrients_fig = px.bar(mean_nutrients.melt(id_vars='Crop', value_vars=['Nitrogen', 'Phosphorus', 'Potassium']),
                           x='Crop', y='value', color='variable',
                           barmode='group',
                           title='bar plot shows the average levels of Nitrogen, Phosphorus, and Potassium for each crop',
                           labels={'variable': 'Nutriment', 'value': 'Niveau Moyen'},
                           color_discrete_sequence=px.colors.qualitative.Set3)
bar_nutrients_fig.update_traces(texttemplate='%{y:.2f}', textposition='outside')
bar_nutrients_fig.show()

"""To identify patterns in the relationships between different features, helping you understand which variables might impact crop recommendations."""

#Heatmap: Correlation between features.
correlation_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)
plt.title('Heatmap: Correlation Matrix')
plt.show()

"""To understand how the pH value is distributed for each crop and whether specific crops prefer certain pH ranges."""

#Histogram: pH value distribution per crop.
hist_fig = px.histogram(df, x='pH_Value', color='Crop', nbins=20,
                        title='Histogram: pH Value Distribution by Crop',
                        color_discrete_sequence=px.colors.qualitative.Pastel)
hist_fig.show()

"""Relationship between environmental factors (Temperature, Humidity, Rainfall) and crops."""

#3D scatter plot visualizes the relationship between Temperature, Humidity, and Rainfall for each crop.
scatter_3d_fig = px.scatter_3d(df, x='Temperature', y='Humidity', z='Rainfall', color='Crop',
                               size='Nitrogen', title='3D Scatter Plot: Temperature, Humidity, and Rainfall by Crop')
scatter_3d_fig.show()

"""This plot suggests that the data might have a right-skewed distribution with most values concentrated near 0, and the presence of a few higher values that are sparse (outliers)."""

#visualize the spread and distribution of the data and Detect Outliers
data = df.iloc[:, [2, 3]]
sns.distplot(data)

"""**Visualize the distribution of each feature**"""

plt.figure(figsize=(12, 6))
sns.set()
sns.set_style('whitegrid')
sns.set_context('talk')

"""visual representation of how Nitrogen values are spread across the dataset.

helping guide feature engineering or transformation decisions.
"""

sns.distplot(df['Nitrogen'], kde=False, bins=20, color='b')
plt.title('Distribution of Nitrogen')
plt.show()

sns.distplot(df['Phosphorus'], kde=False, bins=20, color='g')
plt.title('Distribution of Phosphorus')
plt.show()

sns.distplot(df['Potassium'], kde=False, bins=20, color='r')
plt.title('Distribution of Potassium')
plt.show()

sns.distplot(df['Temperature'], kde=False, bins=20, color='y')
plt.title('Distribution of Temperature')
plt.show()

sns.distplot(df['Humidity'], kde=False, bins=20, color='c')
plt.title('Distribution of Humidity')
plt.show()

sns.distplot(df['pH_Value'], kde=False, bins=20, color='m')
plt.title('Distribution of pH Value')
plt.show()

sns.distplot(df['Rainfall'], kde=False, bins=20, color='m')
plt.title('Distribution of Rainfall')
plt.show()

"""**Drop Outliers**"""


# boxplot

# histogram
def boxplot(df_features: pd.DataFrame,
            ncols=4,
            hide_xlabel=True,
            hide_ylabel=True,
            ) -> None:
    num_features = len(df_features.columns) - 1
    fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 15))
    axes = axes.flatten()

    for i, col in enumerate(df_features.columns[:-1]):

        sns.boxplot(y=df_features[col], data=df_features, hue='Crop', ax=axes[i],
                    linewidth=0.5, color="#8A2BE2")

        if hide_xlabel:
            axes[i].set_xlabel('')
            axes[i].set_xticks([])
            title = f'{col}'
            axes[i].set_title(title)

        if hide_ylabel:
            axes[i].set_ylabel('')
            axes[i].set_yticks([])

    plt.tight_layout()

    [fig.delaxes(ax) for ax in axes if not ax.has_data()]

    plt.show()


#boxplot(df_features=df,hide_xlabel=False,hide_ylabel=False)

"""**Model building**"""

from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix

X = df.drop('Crop', axis=1)
Y = df['Crop']

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)

"""**Creating and Multiple Training  Model**"""

from sklearn.metrics import classification_report, confusion_matrix
from sklearn.model_selection import KFold, cross_val_score
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import PolynomialFeatures

"""**KNeighborsClassifier Model**"""

from sklearn.neighbors import KNeighborsClassifier

model = KNeighborsClassifier()
model.fit(X_train, y_train)

model.score(X_train, y_train)

y_pred = model.predict(X_test)

y_pred = model.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
# Classification report
print(classification_report(y_test, y_pred))

# K-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, Y, cv=kf, scoring='accuracy')

# Results
print(f"Accuracy for each fold: {scores}")
print(f"Mean Accuracy: {scores.mean():.4f}")

"""**RandomForestClassifier**


"""

from sklearn.ensemble import RandomForestClassifier

modelr = RandomForestClassifier()
modelr.fit(X_train, y_train)

modelr.score(X_train, y_train)

y_pred = modelr.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
# Classification report
print(classification_report(y_test, y_pred))

"""K-Fold Cross-Validation"""

# K-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(modelr, X, Y, cv=kf, scoring='accuracy')

# Results
print(f"Accuracy for each fold: {scores}")
print(f"Mean Accuracy: {scores.mean():.4f}")

"""**Naive Bayes GaussianNB**

"""

from sklearn.naive_bayes import GaussianNB

gnb = GaussianNB()
gnb.fit(X_train, y_train)

gnb.score(X_train, y_train)

y_pred = gnb.predict(X_test)
# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")
# Classification report
print(classification_report(y_test, y_pred))

# K-Fold Cross Validation
kf = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(gnb, X, Y, cv=kf, scoring='accuracy')

# Results
print(f"Accuracy for each fold: {scores}")
print(f"Mean Accuracy: {scores.mean():.4f}")

"""**Combine and Select the Best Model**"""

from sklearn.ensemble import VotingClassifier

# Initialize the models
models = {
    'KNN': KNeighborsClassifier(),
    'RandomForest': RandomForestClassifier(random_state=42),
    'NaiveBayes': GaussianNB()
}

# Train and evaluate each model
accuracies = {}
for model_name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    accuracies[model_name] = accuracy
    print(f"{model_name} Accuracy: {accuracy:.4f}")

# Select the best model based on accuracy
best_model_name = max(accuracies, key=accuracies.get)
print(f"Best model: {best_model_name} with Accuracy: {accuracies[best_model_name]:.4f}")

# Optionally, combine models using a Voting Classifier (for ensemble learning)
best_model = models[best_model_name]
voting_model = VotingClassifier(estimators=[
    ('KNN', models['KNN']),
    ('RandomForest', models['RandomForest']),
    ('NaiveBayes', models['NaiveBayes'])
], voting='hard')

# Train and evaluate the Voting Classifier
voting_model.fit(X_train, y_train)
y_pred = voting_model.predict(X_test)
voting_accuracy = accuracy_score(y_test, y_pred)
print(f"Voting Classifier Accuracy: {voting_accuracy:.4f}")

"""1.   Enhances model capability to learn complex relationships.
2.   lead to overfitting, especially if used with higher degrees or in datasets with many features.

**Apply Polynomial Features KNeighborsClassifier**
"""

# Apply Polynomial Features (degree=2, for quadratic features)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, Y, test_size=0.3, random_state=42)
model = KNeighborsClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy with Polynomial Features: {accuracy:.4f}")

"""**Apply Polynomial Features RandomForestClassifier**"""

# Apply Polynomial Features (degree=2, for quadratic features)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, Y, test_size=0.3, random_state=42)
model = RandomForestClassifier()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy with Polynomial Features: {accuracy:.4f}")

"""**Apply Polynomial Features GaussianNB**"""

# Apply Polynomial Features (degree=2, for quadratic features)
poly = PolynomialFeatures(degree=2)
X_poly = poly.fit_transform(X)
X_train, X_test, y_train, y_test = train_test_split(X_poly, Y, test_size=0.3, random_state=42)
gnb = GaussianNB()
gnb.fit(X_train, y_train)
y_pred = model.predict(X_test)
# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy with Polynomial Features: {accuracy:.4f}")

"""1.   Use Bagging when you want to reduce overfitting and stabilize predictions.
2.   Use Boosting when you aim to increase model accuracy by reducing bias.

Bagging with Random Forest
"""

from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_val_score

# Bagging Classifier with Random Forest
bagging_model = BaggingClassifier(
    estimator=RandomForestClassifier(random_state=42),
    n_estimators=50,
    random_state=42
)

# Train the Bagging Model
bagging_model.fit(X_train, y_train)

# Evaluate the Bagging Model
y_pred_bagging = bagging_model.predict(X_test)
bagging_accuracy = accuracy_score(y_test, y_pred_bagging)
print(f"Bagging Model Accuracy: {bagging_accuracy:.4f}")

# Cross-Validation Accuracy
bagging_scores = cross_val_score(bagging_model, X, Y, cv=5, scoring='accuracy')
print(f"Bagging Cross-Validation Mean Accuracy: {bagging_scores.mean():.4f}")

"""Gradient Boosting"""

from sklearn.ensemble import GradientBoostingClassifier

# Gradient Boosting Model
boosting_model = GradientBoostingClassifier(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    random_state=42
)

# Train the Gradient Boosting Model
boosting_model.fit(X_train, y_train)

# Evaluate the Gradient Boosting Model
y_pred_boosting = boosting_model.predict(X_test)
boosting_accuracy = accuracy_score(y_test, y_pred_boosting)
print(f"Gradient Boosting Model Accuracy: {boosting_accuracy:.4f}")

"""XGBoost"""

from xgboost import XGBClassifier

# XGBoost Model
xgb_model = XGBClassifier(
    n_estimators=100,  #Specifies the number of trees (or boosting rounds) in the model.
    learning_rate=0.1,  #Controls the step size during each boosting step
    max_depth=3,  #Limits the depth of each tree
    random_state=42  #fixing the randomness in splitting data, shuffling, same output.
)

# Train the XGBoost Model
xgb_model.fit(X_train, y_train)

# Evaluate the XGBoost Model
y_pred_xgb = xgb_model.predict(X_test)
xgb_accuracy = accuracy_score(y_test, y_pred_xgb)
print(f"XGBoost Model Accuracy: {xgb_accuracy:.4f}")

"""Comparison of Bagging and Boosting Models"""

print("\nModel Performance Comparison:")
print(f"Bagging Accuracy: {bagging_accuracy:.4f}")
print(f"Gradient Boosting Accuracy: {boosting_accuracy:.4f}")
print(f"XGBoost Accuracy: {xgb_accuracy:.4f}")

# import pickle
# # assume your fitted model or pipeline is in the variable `model`
# with open("crop_model.pkl", "wb") as f:
#     pickle.dump(model, f)

# from google.colab import files
# files.download("crop_model.pkl")

test = {
    "nitrogen": 90,
    "phosphorus": 42,
    "potassium": 43,
    "temperature": 20.87974371,
    "humidity": 82.00274423,
    "ph": 6.502985292,
    "rainfall": 202.9355362,
}

# from sklearn.pipeline import Pipeline
# from sklearn.preprocessing import PolynomialFeatures, StandardScaler
# from sklearn.ensemble import RandomForestClassifier
# import joblib
# from google.colab import files

# # 1) Build the pipeline on your raw 7‑feature X_train
# pipe = Pipeline([
#     ("poly", PolynomialFeatures(degree=2, include_bias=True)),
#                # remove this line if unused
#     ("clf", RandomForestClassifier(n_estimators=100, random_state=42)),
# ])

# pipe.fit(X_train, y_train)

# # 2) Grab the poly step
# poly = pipe.named_steps["poly"]

# # 3) Print input/output dims correctly
# print("poly.n_features_in_ =", poly.n_features_in_)  # should be 7
# if hasattr(poly, "n_output_features_"):
#     print("poly.n_output_features_ =", poly.n_output_features_)
# else:
#     feats = poly.get_feature_names_out(FEATURE_COLS)
#     print("poly.n_output_features_ =", len(feats))

# # 4) Export
# joblib.dump(pipe, "crop_pipeline.joblib")
# files.download("crop_pipeline.joblib")
